---
title: "My_own_Project"
author: "Yassin Zeraoulia"
date: "21 09 2021"
output: word_document
---

#Introduction

Stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.
This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, heart diseases, smoking status and other relevant clinical conditions. Each row in the data provides some information about the patient.

VAriable inside the data set:
1) id: unique identifier
2) gender: "Male", "Female" or "Other"
3) age: age of the patient
4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension
5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease
6) ever_married: "No" or "Yes"
7) work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"
8) Residence_type: "Rural" or "Urban"
9) avg_glucose_level: average glucose level in blood
10) bmi: body mass index
11) smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"*
12) stroke: 1 if the patient had a stroke or 0 if not

In order to predict  if a person is more likely to have a stroke we'll have to take into consideration relevant variables of the data set, some of them are not relevant because they don't have impact on the physiology of the human being.
Furthermore we'll have to explore our data set and gain some insight with the use of plots and correlation table.
After the exploration of the data set we can make some assumption on the relevance of each variable and start thinking about a model able to fit correctly our train data once we divided our data set into train a and test set.

The first model that will be build is based on a classification tree model by wich I'll try to estimate if a patiant has high glucose levels (glucose_cat_tr) using as predictors: "stroke",  "hypertension", "heart_disease","bmi_cat_tr".

The second model that will be implemented is a logistic regression used to predict strokes, this model uses as predictors avg_glucose_level, age, heart_disease, and hypertension.



```{r Introduction, loading data set and libraries}
#Loading the data set and libraries

library(tidyverse)
library(dslabs)
library(ggplot2)
library(caret)
library(readr)
library(dplyr)
library(corrplot)
library(reshape2)
library(ggplot2)
library(rsample)
library(caret)
library(data.table)
library(FactoMineR)
library(viridis)
library(rattle)
library(mice)
library(VIM)
library(VGAM)
library(pROC)

#data exploration

healthcare_dataset_stroke_data <- read_csv("healthcare-dataset-stroke-data.csv")

data <- as.data.frame(healthcare_dataset_stroke_data)

str(data)

data <- unique(data)
data <- na.omit(data)

knitr::opts_chunk$set(echo = TRUE)
```

We start by removing unnecessary variable inside the dataset and turning some variables in a preferable data type.



```{r Select relevant variable}

#Remove unnecessary columns from the dataset and changing some data type 

data <- data %>% select( "gender", "age", "hypertension", "heart_disease", "Residence_type","avg_glucose_level", "bmi", "smoking_status","stroke")


data$gender <- as.factor(data$gender)
data$smoking_status <- as.factor(data$smoking_status)
data$Residence_type <- as.factor(data$Residence_type)


knitr::opts_chunk$set(echo = TRUE)
```

#Data exploration 

Now we can gain some initial insight on the variables inside the data set by generating some plots and analizyng distributions  of the data set.

The first plot gives us some idea on the distribution of strokes by age and gender, is clear that there is a problem in the data points gathered because around the age of 25 there is an increase of strokes in the "other" gender category. This can have an impact on the estimation of correlation between variables and the performance of the models that will be built.

The second plot rappresent the correlation between tree variables: heart disease, hypertension and age. 

The third plot is a correlation between bmi and avg_glucose_level.



```{r plots }


#Data exploration 

data %>% ggplot(aes(age,fill = gender)) +
  geom_histogram(alpha =0.5, aes(y = ..density..),col="black", bins = 10) +
  theme(legend.title=element_text(family="Times",size=20),
        legend.text=element_text(family="Times",face ="italic",size=15),
        plot.title=element_text(family="Times", face="bold", size=20),
        axis.title.x=element_text(family="Times", face="bold", size=12),
        axis.title.y=element_text(family="Times", face="bold", size=12)) +
  xlab("age") +
  ylab("cases") +
  ggtitle("Strokes distribution per age and gender")

data$avg_glucose_level <- as.numeric(data$avg_glucose_level)
data$bmi <- as.numeric(data$bmi)
#as we can see there is some  positive correlation between age, hypertention and heart disease
#From this plot the correletion dosen't seems to bee as high as expected 

corrplot(cor(data[,2:4],method ="pearson"),diag =FALSE,
         title ="Correlation Plot", method ="ellipse",
         tl.cex =0.7, tl.col ="black", cl.ratio =0.2
)

#as expected even between bmi and glucose levels there is a slightly positive correlation

corrplot(cor(data[,6:7],method ="pearson", use = "na.or.complete"),diag =FALSE,
         title ="Correlation Plot", method ="ellipse",
         tl.cex =0.7, tl.col ="black", cl.ratio =0.2
)

knitr::opts_chunk$set(echo = TRUE)
```

The variable describing the smoking status could be really helpful, the problem here is that the percentage of strokes in the data set is really small so is not really easy to notice a correlation between stroke and smoking status. Some patiant who had a stroke were not smokers, were not old, had no heart disease or hypertension; this make building a predictive model using this data set not really easy. 

```{r plot smoking status/stroke  and age strokes with colour rappresenting smoking status}

data %>% ggplot(aes(stroke, fill = smoking_status)) +
  geom_bar(alpha =2, col="black") +
  theme(legend.title=element_text(family="Times",size=20),
        legend.text=element_text(family="Times",face ="italic",size=15),
        plot.title=element_text(family="Times", face="bold", size=20),
        axis.title.x=element_text(family="Times", face="bold", size=12),
        axis.title.y=element_text(family="Times", face="bold", size=12)) +
  xlab("stroke") +
  ylab("cases") +
  ggtitle("Strokes and smoking status")

data %>% ggplot(aes(age, fill = smoking_status)) +
  geom_density(alpha =0.5, stat = "count", na.rm = TRUE, width = 5, position = "stack") +
  theme(legend.title=element_text(family="Times",size=20),
        legend.text=element_text(family="Times",face ="italic",size=15),
        plot.title=element_text(family="Times", face="bold", size=20),
        axis.title.x=element_text(family="Times", face="bold", size=12),
        axis.title.y=element_text(family="Times", face="bold", size=12)) +
  xlab("age") +
  ylab("cases") +
  ggtitle("Age and smoking status")

knitr::opts_chunk$set(echo = TRUE)
```

The distribution of bmi is right skewed (long tail to the right). Because this is the only variable with missing data (at least of the numerical variables) we can impute the median on the missing data without losing too much information. 

Only 5% of the people inside the data set had a stroke, This means that our baseline dummy model has an accuracy of 95%.That is if we would predict a person to not have a stroke all the time.

The distribution of avg_glucose_level and bmi dosen't seem to be normal as we cann asses with plot and the Shapiro-Wilk normality test.

```{r}

data <- as.data.table(data)

str(data)

#The histogram of glucose leveles and bmi is not normally distributed in a 
#traditional bell-shape and the Q-Q plot poorly resembles a straight y = x line.

par(mfrow = c(1,2))
hist(data$avg_glucose_level, 100)
qqnorm(data$avg_glucose_level)

par(mfrow = c(1,2))
hist(data$bmi, 100)
qqnorm(data$bmi)

#Shapiro-Wilk normality test, we see the p-value is significant, and thus we reject the null hypothesis of normal data

shapiro.test(data$bmi)
shapiro.test(data$avg_glucose_level[1:5000])
knitr::opts_chunk$set(echo = TRUE)

```

This plot rappresent the missing values inside the data set:

```{r missing values}
#Missing data in the data set

aggr(data, prop = TRUE,
    numbers = TRUE)
knitr::opts_chunk$set(echo = TRUE)
```

In order to build our  model we'll split the data set in two chunk, train (80%) and test (20%)

```{r}
#Split data into train and test set

data1 <- initial_split(data = data, prop = 0.8)


data_train <- training(data1)
data_test <- testing(data1)

knitr::opts_chunk$set(echo = TRUE)
```

The following plots give us some insight: in general stroke happens starting from the age of 40, furthermore we can see the yellow point rappresenting people with hypertension (first plot) and heart disease (second plot).

```{plots}


p1 <- ggplot(data = data_train, aes(x = stroke, y= age))
p1 + geom_point(aes(colour = data_train$hypertension)) +
  scale_colour_viridis(discrete = FALSE)

p1 <- ggplot(data = data_train, aes(x = stroke, y= age))
p1 + geom_point(aes(colour = data_train$heart_disease)) +
  scale_colour_viridis(discrete = FALSE)


knitr::opts_chunk$set(echo = TRUE)
```

#Classification tree

With this model the objective was not to predict strokes, I wanted to predict average glucose level using as predictor stroke, hypertension, bmi and heart disease of patients after turning avg_glucose_level and bmi into factor with four levels.
The results are not great because the model was able to classify the patient into the right category of glucose levels with an efficacy of around 41%.


```{classification tree}


range(data_train$avg_glucose_level)

data_train$bmi <- as.numeric(data_train$bmi)

data_train$glucose_cat_tr <- cut(data_train$avg_glucose_level, breaks = c(55,80,110,150,271), labels = c("low","normal","high","very high"))
data_train$bmi_cat_tr <- cut(data_train$bmi, breaks = c(0, 18, 24,29, 100), labels = c("underweight", "normal","overweight","obese") )

data_test$glucose_cat_ts <- cut(data_test$avg_glucose_level, breaks = c(55,80,110,150,271), labels = c("low","normal","high","very high"))
data_test$bmi_cat_ts <- cut(data_test$bmi, breaks = c(0, 18, 24,29, 100), labels = c("underweight", "normal","overweight","obese") )

set.seed(12345)
cartModel <- train(x = data_train[, c("stroke",  "hypertension", "heart_disease","bmi_cat_tr")],
                   y = factor(data_train$glucose_cat_tr),
                   method = "rpart",
                   preProcess = NULL,
                   tuneLength = 10,
                   trControl = trainControl(method = "cv",
                                            number = 6
                   )
)

cartModel

plot(cartModel$finalModel)
text(cartModel$finalModel, cex = 0.5)
fancyRpartPlot(cartModel$finalModel, cex = 0.4, main = "")

knitr::opts_chunk$set(echo = TRUE)
```

#Logistic regression model predicting strokes 

After fitting the train set to the logistic model it was able to predict the strokes with an accuracy of about 71%.

```{r logistic regression }


m.lr <- glm(stroke ~ avg_glucose_level+age+heart_disease+hypertension,
             family = binomial(link = "logit"),
             data = data_train, model = TRUE)
summary(m.lr)

cut_off <-roc(response= data_train$stroke, predictor= m.lr$fitted.values)


e <-cbind(cut_off$thresholds,cut_off$sensitivities+cut_off$specificities)
best_t <-subset(e,e[,2]==max(e[,2]))[,1]
#Plot ROC Curve
plot(1-cut_off$specificities,cut_off$sensitivities,type="l",
     ylab="Sensitivity",xlab="1-Specificity",col="red",lwd=3,
     main ="ROC Curve for Train")
abline(a=0,b=1)
abline(v = best_t) #add optimal t to ROC curve

cat(" The best value of cut-off for classifier is ", best_t)

# Predict the probabilities for test and apply the cut-off
predict_prob <-predict(m.lr, newdata=data_test, type="response")
#Apply the cutoff to get the class
class_pred <-ifelse(predict_prob >0.045,1,0)
#Classification table
table(data_test$stroke,class_pred)

#Classification rate
sum(diag(table(data_test$stroke,class_pred))/nrow(data_test))

#with a logistic regression model we reached 67% good classification on the test data

anova(m.lr, test="Chisq")

#The chi-square test on four of the variables is significant as the p-value is less than 0.05. 
#4 out of five contributions to the model are significant.

knitr::opts_chunk$set(echo = TRUE)
```

#Conclusion

The objective of building a model that can detect true positive was achived, even though the accuracy is not excellent (71%) the model performed relatively well given that inside the data set only 5% of patients had a stroke and many of them didn't  have hypertension, heart disease and other clinical variable weren't typical of a patient with high probability of strokes.